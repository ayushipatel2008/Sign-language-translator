**Hand Gesture Recognition** is nothing but a logical evolution from touchscreens. Snap your fingers and make your coffee maker brew you a fresh cup. Wave a hand near your smart TV and switch on today’s weather forecast. Apart from this fun and entertainment purposes, It aids individuals with disabilities by offering an alternative means of communication and control. It is also used in Medical care to reduce physical touch. As for automation - BMW’s gesture control system allows drivers to adjust volume, answer calls, or navigate using gestures. And Apple Vision pro is one of the examples that shows hand gesture recognition’s contribution in Virtual and Augmented Reality.


# Sign-language-translator
The goal of this project is to design and implement a real-time sign language  translator that can recognize and translate sign language gestures into spoken language or  text. The system will use Raspberry Pi 4B, sensors, and machine learning  algorithms for gesture recognition. 

Run the source code for model file on Jupyter Notebook. Make sure to choose the correct version of python - 2.4 as my Raspberry Pi 4B had python 2.4 as it's latest version. You can update your CPU or code accordingly. 

Once the gesture_model.h5 or gesture_model.keras file is generated, run the real_time_detection file and your video streaming will begin. 


**Proposed System**
![image](https://github.com/user-attachments/assets/d8441cbf-7199-4dd5-ae09-3da331e9ad2b)

Dataset - https://drive.google.com/file/d/1jF45Ud5cWhGuLeakHtMagQ6usAVOZfUv/view?usp=sharing

Results


